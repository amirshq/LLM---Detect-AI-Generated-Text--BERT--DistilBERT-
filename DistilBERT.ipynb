{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has been created as part of the LLM - Detect AI Generated Text competition from Kaggle.\n",
    "\n",
    "The competition dataset comprises about 10,000 essays, some written by students and some generated by a variety of large language models (LLMs). The goal of the competition is to determine whether or not essay was generated by an LLM.\n",
    "\n",
    "All of the essays were written in response to one of seven essay prompts. In each prompt, the students were instructed to read one or more source texts and then write a response. This same information may or may not have been provided as input to an LLM when generating an essay.\n",
    "\n",
    "Essays from two of the prompts compose the training set; the remaining essays compose the hidden test set. Nearly all of the training set essays were written by students, with only a few generated essays given as examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Benefits of this project is working with Transformers specially with BERT and DistilBERT which is developed by Google at 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Talk about the DataSet:\n",
    "\n",
    "DATASET\n",
    "`test|train_essays.csv`\n",
    "\n",
    "`id` - A unique identifier for each essay.\n",
    "\n",
    "\n",
    "`prompt_id` - Identifies the prompt the essay was written in response to.\n",
    "\n",
    "`text` - The essay text itself.\n",
    "\n",
    "`generated` - Whether the essay was written by a student (0) or generated by an LLM (1). This field is the target and is not present in test_essays.csv.\n",
    "\n",
    "train_prompts.csv - Essays were written in response to information in these fields.\n",
    "\n",
    "`prompt_id` - A unique identifier for each prompt.\n",
    "\n",
    "`prompt_name` - The title of the prompt. instructions - The instructions given to students.\n",
    "\n",
    "`source_text` - The text of the article(s) the essays were written in response to, in Markdown format. Significant paragraphs are enumerated by a numeral preceding the paragraph on the same line, as in 0 Paragraph one.\\n\\n1 \n",
    "`Paragraph two`.. Essays sometimes refer to a paragraph by its numeral. Each article is preceded with its title in a heading, like # Title. When an author is indicated, their name will be given in the title after by. Not all articles have authors indicated. An article may have subheadings indicated like ## Subheading.\n",
    "\n",
    "`sample_submission.csv` - A submission file in the correct format. See the Evaluation page for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\amirs\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Using TensorFlow backend\n",
      "Keras Version: 0.1.7\n",
      "Tensorflow Version: 2.15.0\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import spacy\n",
    "import regex as re\n",
    "\n",
    "#Function to Plot Wordcloud\n",
    "#A word cloud is a visualization of word frequency where words that appear more \n",
    "#frequently in the text are displayed with larger fonts. \n",
    "#You can use this class to generate word clouds from your textual data.\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from collections import Counter\n",
    "\n",
    "#Tensorflow/KERAS\n",
    "import tensorflow as tf \n",
    "import keras_core as keras \n",
    "from keras import layers,Sequential\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau,CSVLogger,LearningRateScheduler\n",
    "\n",
    "#Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,classification_report,precision_recall_curve,roc_curve,auc \n",
    "\n",
    "#Set the backend for Keras\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tf\"\n",
    "\n",
    "#set seed for reproductibility \n",
    "keras.utils.set_random_seed(42)\n",
    "\n",
    "#Use mixed precision to speed up all training\n",
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "#Check Versions \n",
    "print(f'Keras Version: {keras.__version__}')\n",
    "print(f'Tensorflow Version: {tf.__version__}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided function compress(`df`, `verbose=True`) is a comprehensive function that reduces the memory usage of a DataFrame by downcasting numerical columns and provides detailed information about the optimization process. Here's a breakdown of what the function does:\n",
    "\n",
    "`df`: This is the input DataFrame that you want to compress.\n",
    "\n",
    "`verbose`: This is an optional parameter that determines whether the function should print information about the size reduction. By default, it's set to True, meaning it will print information; you can set it to False if you don't want the information to be printed.\n",
    "\n",
    "The function performs the following steps:\n",
    "\n",
    "Calculates the initial memory usage of the DataFrame (`input_size`) in megabytes (MB).\n",
    "\n",
    "If the verbose parameter is set to True, it prints the initial size of the DataFrame.\n",
    "\n",
    "Creates a copy of the original data types of the DataFrame's columns in `dtype_before`.\n",
    "\n",
    "Iterates through each numerical column in the DataFrame (columns with data types '`float64`' and '`int64`').\n",
    "\n",
    "For each numerical column, it checks the minimum and maximum values in the column (`col_min` and `col_max`).\n",
    "\n",
    "If the column's data type is '`int64`', it attempts to downcast the column to the smallest integer data type that can hold the range of values in that column (e.g., '`int8`', '`int16`', '`int32`', or '`int64`').\n",
    "\n",
    "If the column's data type is '`float64`', it attempts to downcast the column to a smaller floating-point data type that can represent the range of values in that column (e.g., '`float32`' or '`float64`').\n",
    "\n",
    "The downcasting is done using NumPy data type casting (`np.int8`, `np.int16`, `np.int32`, `np.int6`4, `np.float32`,` np.float64`).\n",
    "\n",
    "After downcasting, the DataFrame's memory usage is reduced because the smaller data types occupy less memory.\n",
    "\n",
    "If verbose is True, it prints the new memory usage of the DataFrame and the percentage reduction in size.\n",
    "\n",
    "It filters the DataFrame to keep only numerical columns for comparison (`numeric_columns`).\n",
    "\n",
    "It creates a copy of data types after compression in dtype_after.\n",
    "\n",
    "It constructs a comparison DataFrame `comparison_df` that shows the data types before and after compression and the size reduction percentage for each numerical column.\n",
    "\n",
    "The function returns the compressed `DataFrame (df)` and the comparison DataFrame (`comparison_df`).\n",
    "\n",
    "This function is useful for optimizing memory usage when working with large DataFrames, especially in situations where efficient memory usage is crucial. It not only reduces memory usage but also provides detailed information about the optimization process for analysis and reporting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Downcast Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Reduces the size of the DataFrame by downcasting numerical columns\n",
    "    \"\"\"\n",
    "    input_size = df.memory_usage(index=True).sum() / (1024 ** 2)\n",
    "    if verbose:\n",
    "        print(\"Old dataframe size:\", round(input_size, 2), 'MB')\n",
    "\n",
    "    in_size = df.memory_usage(index=True).sum()\n",
    "    dtype_before = df.dtypes.copy()  # Copy of original data types\n",
    "\n",
    "    for col in df.select_dtypes(include=['float64', 'int64']):\n",
    "        col_type = df[col].dtype\n",
    "        col_min, col_max = df[col].min(), df[col].max()\n",
    "\n",
    "        if col_type == 'int64':\n",
    "            if col_min > np.iinfo(np.int8).min and col_max < np.iinfo(np.int8).max:\n",
    "                df[col] = df[col].astype(np.int8)\n",
    "            elif col_min > np.iinfo(np.int16).min and col_max < np.iinfo(np.int16).max:\n",
    "                df[col] = df[col].astype(np.int16)\n",
    "            elif col_min > np.iinfo(np.int32).min and col_max < np.iinfo(np.int32).max:\n",
    "                df[col] = df[col].astype(np.int32)\n",
    "            elif col_min > np.iinfo(np.int64).min and col_max < np.iinfo(np.int64).max:\n",
    "                df[col] = df[col].astype(np.int64)\n",
    "        elif col_type == 'float64':\n",
    "            ## float16 warns of overflow\n",
    "            # if col_min > np.finfo(np.float16).min and col_max < np.finfo(np.float16).max:\n",
    "            #     df[col] = df[col].astype(np.float16)\n",
    "            if col_min > np.finfo(np.float32).min and col_max < np.finfo(np.float32).max:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "            elif col_min > np.finfo(np.float64).min and col_max < np.finfo(np.float64).max:\n",
    "                df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    out_size = df.memory_usage(index=True).sum()\n",
    "    ratio = (1 - round(out_size / in_size, 2)) * 100\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Optimized size by {}%\".format(round(ratio, 2)))\n",
    "        print(\"New DataFrame size:\", round(out_size / (1024 ** 2), 2), \"MB\")\n",
    "\n",
    "    # Filter only numerical columns for comparison\n",
    "    numeric_columns = df.select_dtypes(include=['float32', 'float64', 'int8', 'int16', 'int32', 'int64'])\n",
    "    dtype_after = numeric_columns.dtypes.copy()  # Copy of data types after compression\n",
    "    \n",
    "    # Create a comparison DataFrame\n",
    "    comparison_df = pd.DataFrame({'Before': dtype_before[numeric_columns.columns], 'After': dtype_after})\n",
    "    comparison_df['Size Reduction'] = ratio\n",
    "\n",
    "    return df, comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordCloud Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate WordCloud\n",
    "\n",
    "def generate_wordcloud_subplot(df, label_value, subplot_position, max_words=1000, width=800, height=400, top_n = 10):\n",
    "    \"\"\"\n",
    "    Generate a word cloud for a specific label value and display it in a subplot.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing text data and labels.\n",
    "        label_value (int): The label value for which to generate the word cloud.\n",
    "        subplot_position (int): The position of the subplot where the word cloud will be displayed.\n",
    "        max_words (int, optional): Maximum number of words to include in the word cloud. Default is 1000.\n",
    "        width (int, optional): Width of the word cloud image. Default is 800.\n",
    "        height (int, optional): Height of the word cloud image. Default is 400.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Select the text subset for the specified label value\n",
    "    text_subset = df[df.generated == label_value].text\n",
    "\n",
    "    # Define stopwords to be excluded\n",
    "    stopwords = set(STOPWORDS)\n",
    "\n",
    "    # Create a WordCloud object with specified parameters\n",
    "    wc = WordCloud(max_words=max_words, width=width, height=height, stopwords=stopwords)\n",
    "\n",
    "    # Generate the word cloud from the selected text subset\n",
    "    wc.generate(\" \".join(text_subset))\n",
    "\n",
    "    # Create a subplot and display the word cloud\n",
    "    plt.subplot(subplot_position)\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "\n",
    "    # Set the title for the word cloud plot\n",
    "    title = f'WordCloud for Label {label_value} ({(\"Student\" if label_value == 0 else \"AI\")})'\n",
    "    plt.title(title)\n",
    "    \n",
    "    # Count occurrences of words in the text subset\n",
    "    words_count = Counter(\" \".join(text_subset).split())\n",
    "    top_words = words_count.most_common(top_n)\n",
    "    bottom_words = words_count.most_common()[:-top_n-1:-1]  # Extract least common words\n",
    "\n",
    "    # Print the most common words\n",
    "    print(f\"Top {top_n} words for Label {label_value}:\")\n",
    "    for idx, (word, count) in enumerate(top_words, start=1):\n",
    "        print(f\"{idx}. {word}: {count} times\")\n",
    "    print(\"------------------------------\")\n",
    "\n",
    "    # Print the least common words\n",
    "    print(f\"Least {top_n} words for Label {label_value}:\")\n",
    "    for idx, (word, count) in enumerate(bottom_words, start=1):\n",
    "        print(f\"{idx}. {word}: {count} times\")\n",
    "    print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 files which contains the following information:\n",
    "\n",
    "`test|train_essays.csv`\n",
    "\n",
    "`id`: A unique identifier for each essay.\n",
    "\n",
    "`prompt_id`: Identifies the prompt the essay was written in response to.\n",
    "\n",
    "`text`: The essay text itself.\n",
    "\n",
    "`generated`: Whether the essay was written by a student (0) or generated by an LLM (1). This field is the target and is not present in `test_essays.csv`.\n",
    "\n",
    "`train_prompts.csv`\n",
    "\n",
    "Essays were written in response to information in these fields.\n",
    "\n",
    "`prompt_id`: A unique identifier for each prompt.\n",
    "\n",
    "`prompt_name`: The title of the prompt.\n",
    "\n",
    "`instructions`: The instructions given to students.\n",
    "\n",
    "`source_text`: The text of the article(s) the essays were written in response to, in Markdown format. Significant paragraphs are enumerated by a numeral preceding the paragraph on the same line, as in 0 Paragraph one.\\n\\n1 Paragraph two. Essays sometimes refer to a paragraph by its numeral. Each article is preceded with its title in a heading, like # Title. When an author is indicated, their name will be given in the title after by. Not all articles have authors indicated. An article may have subheadings indicated like ## Subheading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/sample_submission.csv\n",
      "Data/test_essays.csv\n",
      "Data/train_essays.csv\n",
      "Data/train_prompts.csv\n"
     ]
    }
   ],
   "source": [
    "# Import Data\n",
    "\n",
    "data_path = 'Data/'\n",
    "\n",
    "for dirname, _, filenames in os.walk(data_path):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Essays Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1378 entries, 0 to 1377\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         1378 non-null   object\n",
      " 1   prompt_id  1378 non-null   int64 \n",
      " 2   text       1378 non-null   object\n",
      " 3   generated  1378 non-null   int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 43.2+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0059830c</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars. Cars have been around since they became ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>005db917</td>\n",
       "      <td>0</td>\n",
       "      <td>Transportation is a large necessity in most co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>008f63e3</td>\n",
       "      <td>0</td>\n",
       "      <td>\"America's love affair with it's vehicles seem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00940276</td>\n",
       "      <td>0</td>\n",
       "      <td>How often do you ride in a car? Do you drive a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00c39458</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars are a wonderful thing. They are perhaps o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  prompt_id                                               text  \\\n",
       "0  0059830c          0  Cars. Cars have been around since they became ...   \n",
       "1  005db917          0  Transportation is a large necessity in most co...   \n",
       "2  008f63e3          0  \"America's love affair with it's vehicles seem...   \n",
       "3  00940276          0  How often do you ride in a car? Do you drive a...   \n",
       "4  00c39458          0  Cars are a wonderful thing. They are perhaps o...   \n",
       "\n",
       "   generated  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Data\n",
    "\n",
    "df_train_essays = pd.read_csv(data_path + \"train_essays.csv\")\n",
    "print(df_train_essays.info())\n",
    "df_train_essays.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old dataframe size: 0.04 MB\n",
      "Optimized size by 44.0%\n",
      "New DataFrame size: 0.02 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Before</th>\n",
       "      <th>After</th>\n",
       "      <th>Size Reduction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prompt_id</th>\n",
       "      <td>int64</td>\n",
       "      <td>int8</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>generated</th>\n",
       "      <td>int64</td>\n",
       "      <td>int8</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Before After  Size Reduction\n",
       "prompt_id  int64  int8            44.0\n",
       "generated  int64  int8            44.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_essays,compress_df = compress(df_train_essays,verbose=True)\n",
    "compress_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cars. Cars have been around since they became famous in the 1900s, when Henry Ford created and built the first ModelT. Cars have played a major role in our every day lives since then. But now, people are starting to question if limiting car usage would be a good thing. To me, limiting the use of cars might be a good thing to do.\\n\\nIn like matter of this, article, \"In German Suburb, Life Goes On Without Cars,\" by Elizabeth Rosenthal states, how automobiles are the linchpin of suburbs, where middle class families from either Shanghai or Chicago tend to make their homes. Experts say how this is a huge impediment to current efforts to reduce greenhouse gas emissions from tailpipe. Passenger cars are responsible for 12 percent of greenhouse gas emissions in Europe...and up to 50 percent in some carintensive areas in the United States. Cars are the main reason for the greenhouse gas emissions because of a lot of people driving them around all the time getting where they need to go. Article, \"Paris bans driving due to smog,\" by Robert Duffer says, how Paris, after days of nearrecord pollution, enforced a partial driving ban to clear the air of the global city. It also says, how on Monday, motorist with evennumbered license plates were ordered to leave their cars at home or be fined a 22euro fine 31. The same order would be applied to oddnumbered plates the following day. Cars are the reason for polluting entire cities like Paris. This shows how bad cars can be because, of all the pollution that they can cause to an entire city.\\n\\nLikewise, in the article, \"Carfree day is spinning into a big hit in Bogota,\" by Andrew Selsky says, how programs that\\'s set to spread to other countries, millions of Columbians hiked, biked, skated, or took the bus to work during a carfree day, leaving streets of this capital city eerily devoid of traffic jams. It was the third straight year cars have been banned with only buses and taxis permitted for the Day Without Cars in the capital city of 7 million. People like the idea of having carfree days because, it allows them to lesson the pollution that cars put out of their exhaust from people driving all the time. The article also tells how parks and sports centers have bustled throughout the city uneven, pitted sidewalks have been replaced by broad, smooth sidewalks rushhour restrictions have dramatically cut traffic and new restaurants and upscale shopping districts have cropped up. Having no cars has been good for the country of Columbia because, it has aloud them to repair things that have needed repairs for a long time, traffic jams have gone down, and restaurants and shopping districts have popped up, all due to the fact of having less cars around.\\n\\nIn conclusion, the use of less cars and having carfree days, have had a big impact on the environment of cities because, it is cutting down the air pollution that the cars have majorly polluted, it has aloud countries like Columbia to repair sidewalks, and cut down traffic jams. Limiting the use of cars would be a good thing for America. So we should limit the use of cars by maybe riding a bike, or maybe walking somewhere that isn\\'t that far from you and doesn\\'t need the use of a car to get you there. To me, limiting the use of cars might be a good thing to do.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_essays.text[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
