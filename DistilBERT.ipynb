{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has been created as part of the LLM - Detect AI Generated Text competition from Kaggle.\n",
    "\n",
    "The competition dataset comprises about 10,000 essays, some written by students and some generated by a variety of large language models (LLMs). The goal of the competition is to determine whether or not essay was generated by an LLM.\n",
    "\n",
    "All of the essays were written in response to one of seven essay prompts. In each prompt, the students were instructed to read one or more source texts and then write a response. This same information may or may not have been provided as input to an LLM when generating an essay.\n",
    "\n",
    "Essays from two of the prompts compose the training set; the remaining essays compose the hidden test set. Nearly all of the training set essays were written by students, with only a few generated essays given as examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Benefits of this project is working with Transformers specially with BERT and DistilBERT which is developed by Google at 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Talk about the DataSet:\n",
    "\n",
    "DATASET\n",
    "`test|train_essays.csv`\n",
    "\n",
    "`id` - A unique identifier for each essay.\n",
    "\n",
    "\n",
    "`prompt_id` - Identifies the prompt the essay was written in response to.\n",
    "\n",
    "`text` - The essay text itself.\n",
    "\n",
    "`generated` - Whether the essay was written by a student (0) or generated by an LLM (1). This field is the target and is not present in test_essays.csv.\n",
    "\n",
    "train_prompts.csv - Essays were written in response to information in these fields.\n",
    "\n",
    "`prompt_id` - A unique identifier for each prompt.\n",
    "\n",
    "`prompt_name` - The title of the prompt. instructions - The instructions given to students.\n",
    "\n",
    "`source_text` - The text of the article(s) the essays were written in response to, in Markdown format. Significant paragraphs are enumerated by a numeral preceding the paragraph on the same line, as in 0 Paragraph one.\\n\\n1 \n",
    "`Paragraph two`.. Essays sometimes refer to a paragraph by its numeral. Each article is preceded with its title in a heading, like # Title. When an author is indicated, their name will be given in the title after by. Not all articles have authors indicated. An article may have subheadings indicated like ## Subheading.\n",
    "\n",
    "`sample_submission.csv` - A submission file in the correct format. See the Evaluation page for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\amirs\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Using TensorFlow backend\n",
      "Keras Version: 0.1.7\n",
      "Tensorflow Version: 2.15.0\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import spacy\n",
    "import regex as re\n",
    "\n",
    "#Function to Plot Wordcloud\n",
    "#A word cloud is a visualization of word frequency where words that appear more \n",
    "#frequently in the text are displayed with larger fonts. \n",
    "#You can use this class to generate word clouds from your textual data.\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from collections import Counter\n",
    "\n",
    "#Tensorflow/KERAS\n",
    "import tensorflow as tf \n",
    "import keras_core as keras \n",
    "from keras import layers,Sequential\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau,CSVLogger,LearningRateScheduler\n",
    "\n",
    "#Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,classification_report,precision_recall_curve,roc_curve,auc \n",
    "\n",
    "#Set the backend for Keras\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tf\"\n",
    "\n",
    "#set seed for reproductibility \n",
    "keras.utils.set_random_seed(42)\n",
    "\n",
    "#Use mixed precision to speed up all training\n",
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "#Check Versions \n",
    "print(f'Keras Version: {keras.__version__}')\n",
    "print(f'Tensorflow Version: {tf.__version__}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Reduces the size of the DataFrame by downcasting numerical columns\n",
    "    \"\"\"\n",
    "    input_size = df.memory_usage(index=True).sum() / (1024 ** 2)\n",
    "    if verbose:\n",
    "        print(\"Old dataframe size:\", round(input_size, 2), 'MB')\n",
    "\n",
    "    in_size = df.memory_usage(index=True).sum()\n",
    "    dtype_before = df.dtypes.copy()  # Copy of original data types\n",
    "\n",
    "    for col in df.select_dtypes(include=['float64', 'int64']):\n",
    "        col_type = df[col].dtype\n",
    "        col_min, col_max = df[col].min(), df[col].max()\n",
    "\n",
    "        if col_type == 'int64':\n",
    "            if col_min > np.iinfo(np.int8).min and col_max < np.iinfo(np.int8).max:\n",
    "                df[col] = df[col].astype(np.int8)\n",
    "            elif col_min > np.iinfo(np.int16).min and col_max < np.iinfo(np.int16).max:\n",
    "                df[col] = df[col].astype(np.int16)\n",
    "            elif col_min > np.iinfo(np.int32).min and col_max < np.iinfo(np.int32).max:\n",
    "                df[col] = df[col].astype(np.int32)\n",
    "            elif col_min > np.iinfo(np.int64).min and col_max < np.iinfo(np.int64).max:\n",
    "                df[col] = df[col].astype(np.int64)\n",
    "        elif col_type == 'float64':\n",
    "            ## float16 warns of overflow\n",
    "            # if col_min > np.finfo(np.float16).min and col_max < np.finfo(np.float16).max:\n",
    "            #     df[col] = df[col].astype(np.float16)\n",
    "            if col_min > np.finfo(np.float32).min and col_max < np.finfo(np.float32).max:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "            elif col_min > np.finfo(np.float64).min and col_max < np.finfo(np.float64).max:\n",
    "                df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    out_size = df.memory_usage(index=True).sum()\n",
    "    ratio = (1 - round(out_size / in_size, 2)) * 100\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Optimized size by {}%\".format(round(ratio, 2)))\n",
    "        print(\"New DataFrame size:\", round(out_size / (1024 ** 2), 2), \"MB\")\n",
    "\n",
    "    # Filter only numerical columns for comparison\n",
    "    numeric_columns = df.select_dtypes(include=['float32', 'float64', 'int8', 'int16', 'int32', 'int64'])\n",
    "    dtype_after = numeric_columns.dtypes.copy()  # Copy of data types after compression\n",
    "    \n",
    "    # Create a comparison DataFrame\n",
    "    comparison_df = pd.DataFrame({'Before': dtype_before[numeric_columns.columns], 'After': dtype_after})\n",
    "    comparison_df['Size Reduction'] = ratio\n",
    "\n",
    "    return df, comparison_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
